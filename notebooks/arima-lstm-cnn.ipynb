{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256f6157",
   "metadata": {},
   "source": [
    "## Hourly Energy Consumption Forecasting with ARIMA-LSTM-CNN Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8baee",
   "metadata": {},
   "source": [
    "**Introduction to the ARIMA-LSTM-CNN Hybrid Model**  \n",
    "The **ARIMA-LSTM-CNN hybrid model** is a sophisticated forecasting approach designed to leverage the complementary strengths of statistical and deep learning techniques. Here’s a breakdown of its components and workflow logic:  \n",
    "\n",
    "- **ARIMA (AutoRegressive Integrated Moving Average):**  \n",
    "  A classical time series model that captures **linear patterns** (trends, seasonality, autocorrelation) in the data. It serves as the \"first layer\" to model predictable linear relationships, leaving residuals (errors) that may contain nonlinear or complex patterns.  \n",
    "\n",
    "- **LSTM (Long Short-Term Memory):**  \n",
    "  A type of recurrent neural network (RNN) adept at modeling **temporal dependencies** in sequential data. It processes the residuals from ARIMA to learn long-term nonlinear relationships in the time series.  \n",
    "\n",
    "- **CNN (Convolutional Neural Network):**  \n",
    "  Used for **local feature extraction** from sequential data (e.g., detecting short-term patterns or anomalies in residuals). When combined with LSTM, it enhances the model’s ability to identify hierarchical spatial-temporal structures.  \n",
    "\n",
    "**Why Hybridize?**  \n",
    "By decomposing the forecasting task into linear (ARIMA) and nonlinear (LSTM-CNN) components, the model addresses the limitations of standalone methods. ARIMA handles baseline trends, while the LSTM-CNN duo refines the residuals, capturing intricate patterns that linear models miss. The final forecast is a sum of ARIMA’s prediction and the LSTM-CNN residual correction, often yielding superior accuracy compared to individual models.  \n",
    "\n",
    "This approach is particularly effective for complex time series (e.g., energy demand) where both linear trends and nonlinear interactions (e.g., weather effects, holiday anomalies) coexist. The workflow below details its implementation steps.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37283f8",
   "metadata": {},
   "source": [
    "**Workflow Overview**\n",
    "\n",
    "1.  **Data Loading & Preprocessing:** Load PJME hourly energy data and weather data, perform feature engineering (calendar features, holidays, weather averaging).\n",
    "2.  **Train/Validation/Test Split:** Split data chronologically.\n",
    "3.  **ARIMA Modeling:** Fit an ARIMA model to capture linear patterns in the time series (or its residuals).\n",
    "4.  **Residual Analysis:** Obtain residuals from the ARIMA model (Actual - ARIMA Prediction).\n",
    "5.  **LSTM-CNN Data Preparation:** Prepare sequences from ARIMA residuals and relevant features for the deep learning model.\n",
    "6.  **LSTM-CNN Model Definition:** Define a hybrid model combining LSTM (for temporal dependencies) and CNN (for feature extraction from sequences).\n",
    "7.  **LSTM-CNN Training:** Train the LSTM-CNN model on the prepared sequences to predict the ARIMA residuals.\n",
    "8.  **Combined Forecasting:** Generate final forecasts by adding ARIMA predictions and LSTM-CNN predictions (of residuals).\n",
    "9.  **Evaluation:** Evaluate the combined forecast using RMSE, MAE, and MAPE on the test set.\n",
    "10. **Visualization:** Plot actual vs. predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Libraries ---\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# --- Data Manipulation ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# --- Machine Learning ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# --- Time Series Modeling ---\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# --- PyTorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Weather Data ---\n",
    "from meteostat import Stations, Hourly\n",
    "\n",
    "# --- Configure Warnings and Plot Style ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# --- Check for GPU Availability ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f81e5",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data (Energy + Weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PJME hourly energy consumption data\n",
    "pjme = (\n",
    "    pd.read_csv(\n",
    "        '../data/3/PJME_hourly.csv',\n",
    "        index_col='Datetime',\n",
    "        parse_dates=['Datetime']\n",
    "    )\n",
    "    .sort_index()\n",
    "    .loc['2003-01-01':'2018-08-02']\n",
    ")\n",
    "\n",
    "# --- Feature Engineering: Holidays and Calendar Features ---\n",
    "cal = USFederalHolidayCalendar()\n",
    "fed_hols = cal.holidays(start=pjme.index.min(), end=pjme.index.max())\n",
    "extended_hols = set(fed_hols)\n",
    "for year in range(pjme.index.year.min(), pjme.index.year.max() + 1):\n",
    "    july4 = datetime(year, 7, 4)\n",
    "    wd = july4.weekday()\n",
    "    if wd == 1: extended_hols.add(july4 - timedelta(days=1))\n",
    "    elif wd == 2: extended_hols.add(july4 + timedelta(days=1)); extended_hols.add(july4 + timedelta(days=2))\n",
    "    elif wd == 3: extended_hols.add(july4 + timedelta(days=1))\n",
    "all_hols = pd.DatetimeIndex(sorted(extended_hols))\n",
    "pjme['is_holiday'] = pjme.index.normalize().isin(all_hols)\n",
    "pjme['is_weekend'] = pjme.index.weekday >= 5\n",
    "pjme['is_dayoff'] = pjme['is_holiday'] | pjme['is_weekend']\n",
    "pjme.drop(columns=['is_holiday', 'is_weekend'], inplace=True)\n",
    "pjme['hour']       = pjme.index.hour\n",
    "pjme['dayofweek']  = pjme.index.weekday\n",
    "pjme['month']      = pjme.index.month\n",
    "pjme['year']       = pjme.index.year\n",
    "pjme['dayofyear']  = pjme.index.dayofyear\n",
    "\n",
    "# --- Fetch and Process Weather Data ---\n",
    "start_dt = datetime(2002, 12, 31)\n",
    "end_dt   = datetime(2018, 8, 4)\n",
    "target_icaos = ['KPHL', 'KEWR', 'KBWI', 'KDCA']\n",
    "stations_query = Stations().nearby(39.95, -75.17).inventory('hourly', (start_dt, end_dt))\n",
    "target_stations_df = stations_query.fetch()[lambda x: x['icao'].isin(target_icaos)]\n",
    "station_ids = target_stations_df.index.tolist()\n",
    "weather_all = Hourly(station_ids, start_dt, end_dt).fetch()\n",
    "weather_cols = ['temp', 'dwpt', 'rhum', 'prcp', 'wspd']\n",
    "average_weather = weather_all.groupby(level='time').mean(numeric_only=True)[weather_cols].ffill()\n",
    "\n",
    "# --- Combine Energy and Weather Data ---\n",
    "pjme_weather = pjme.join(average_weather, how='left').dropna()\n",
    "\n",
    "print(f'Combined data shape: {pjme_weather.shape}')\n",
    "print(f'Index monotonic? {pjme_weather.index.is_monotonic_increasing}')\n",
    "pjme_weather.head()\n",
    "\n",
    "# --- ADD THIS BLOCK TO HANDLE DST DUPLICATES ---\n",
    "if not pjme_weather.index.is_unique:\n",
    "    print(\"WARNING: Duplicate timestamps found in pjme_weather index! (Likely DST)\")\n",
    "    print(\"Resolving by averaging values for duplicate timestamps...\")\n",
    "    # Group by index, calculate mean for all columns, then sort index again\n",
    "    pjme_weather = pjme_weather.groupby(pjme_weather.index).mean()\n",
    "    pjme_weather = pjme_weather.sort_index() # Ensure index is sorted after grouping\n",
    "    print(f\"Shape after averaging duplicates: {pjme_weather.shape}\")\n",
    "    print(f\"Index is now unique: {pjme_weather.index.is_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85dd874",
   "metadata": {},
   "source": [
    "## 2. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f9aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split points (e.g., 80% train, 10% val, 10% test)\n",
    "total_hours = len(pjme_weather)\n",
    "test_split_idx = int(total_hours * 0.9)\n",
    "val_split_idx = int(total_hours * 0.8)\n",
    "\n",
    "train_df = pjme_weather.iloc[:val_split_idx].copy()\n",
    "val_df = pjme_weather.iloc[val_split_idx:test_split_idx].copy()\n",
    "test_df = pjme_weather.iloc[test_split_idx:].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df)} rows ({train_df.index.min()} to {train_df.index.max()})\")\n",
    "print(f\"Val  : {len(val_df)} rows ({val_df.index.min()} to {val_df.index.max()})\")\n",
    "print(f\"Test : {len(test_df)} rows ({test_df.index.min()} to {test_df.index.max()})\")\n",
    "\n",
    "TARGET = 'PJME_MW'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8ae85",
   "metadata": {},
   "source": [
    "## 3. ARIMA Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd86bf5",
   "metadata": {},
   "source": [
    "ARIMA stands for **AutoRegressive Integrated Moving Average**. It’s a class of time-series models that combines:\n",
    "\n",
    "- **AutoRegressive (AR)** terms: regression on its own past values,  \n",
    "- **Integrated (I)** term: differencing the series to make it stationary,  \n",
    "- **Moving Average (MA)** terms: modeling the error as a linear combination of past forecast errors.\n",
    "\n",
    "Fit an ARIMA model to the training data. This captures linear relationships. We might use exogenous variables (weather, calendar) if using ARIMAX. First, we need to analyze the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to determine a suitable order (p, d, q) for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0d26f",
   "metadata": {},
   "source": [
    "### Stationarity Check and Differencing (if needed)\n",
    "\n",
    "ARIMA models require the time series to be stationary. We can check for stationarity using tests like the Augmented Dickey-Fuller (ADF) test or by visual inspection. If non-stationary, we'll difference the series (typically d=1 is sufficient for level non-stationarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform ADF test on the training target variable\n",
    "# adf_result = adfuller(train_df[TARGET])\n",
    "# print(f'ADF Statistic: {adf_result[0]}')\n",
    "# print(f'p-value: {adf_result[1]}')\n",
    "# print('Critical Values:')\n",
    "# for key, value in adf_result[4].items():\n",
    "#     print(f'\\t{key}: {value}')\n",
    "\n",
    "# # If p-value > 0.05, the series is likely non-stationary and needs differencing\n",
    "# if adf_result[1] > 0.05:\n",
    "#     print(\"\\nSeries is likely non-stationary. Applying first difference (d=1).\")\n",
    "\n",
    "# else:\n",
    "#     print(\"\\nSeries is likely stationary. We can proceed without differencing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b592e6",
   "metadata": {},
   "source": [
    "## ACF and PACF Plots\n",
    "\n",
    "### 1. ACF (top panel)\n",
    "\n",
    "- **Lag 1 spike near 1.0** tells you that each hour’s load is almost perfectly correlated with the previous hour.  \n",
    "- **Slow, exponential-style decay** out to around lag 10–15 indicates a strong AR (autoregressive) component.  \n",
    "- **Renewed bump around lag 24–26** flags a daily cycle (24 hr seasonality) still present in the data.\n",
    "\n",
    "### 2. PACF (bottom panel)\n",
    "\n",
    "- **Huge spikes at lag 1 and lag 2**, then a sharp drop to near zero, suggest an AR(2) process (i.e. you need two AR terms to capture the core dynamics).  \n",
    "- **Seasonal spike at lag 24 (and a mirror at 25)** confirms you’ve still got a daily seasonal pattern—so either include a seasonal AR term at lag 24 or difference it out.\n",
    "\n",
    "### What to take from this\n",
    "\n",
    "1. **p ≈ 2** (PACF cuts off after lag 2).  \n",
    "2. **q small** (since ACF tails off rather than “cuts off” sharply, you might try MA(1) or even q=0).  \n",
    "3. **d = 0** here (ADF said stationary), but you could explore a seasonal difference at lag 24 (`D=1`) if you want to knock that daily bump straight out.  \n",
    "4. **Seasonal component**: either explicitly model SARIMA(…,×(P, D, Q, 24)) or use Fourier/exogenous terms for the 24-hour cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc755ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot ACF and PACF for the (potentially differenced) training data\n",
    "# # Adjust 'data_to_plot' if differencing was applied\n",
    "# data_to_plot = train_df[TARGET] # Or train_diff if differenced\n",
    "# lags = 40 # Number of lags to show\n",
    "\n",
    "# fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "# plot_acf(data_to_plot, lags=lags, ax=axes[0], title='Autocorrelation Function (ACF)')\n",
    "# plot_pacf(data_to_plot, lags=lags, ax=axes[1], title='Partial Autocorrelation Function (PACF)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # --- Interpretation Guidance ---\n",
    "# # Based on the plots, choose candidate (p, d, q) orders.\n",
    "# # Example: If PACF cuts off after lag 2 and ACF tails off, suggest AR(2) -> p=2.\n",
    "# # Example: If ACF cuts off after lag 1 and PACF tails off, suggest MA(1) -> q=1.\n",
    "# # If both tail off, consider ARMA(p,q).\n",
    "# # Remember 'd' is the order of differencing needed for stationarity (often 0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) ARIMA / SARIMA orders based on our ADF + ACF/PACF ---\n",
    "arima_order     = (2, 0, 1)\n",
    "seasonal_order  = (1, 0, 0, 24)  # daily seasonality\n",
    "\n",
    "print(f\"Fitting SARIMA{arima_order}×{seasonal_order} model...\")\n",
    "# Fit the SARIMA model on the full training series\n",
    "arima_model   = ARIMA(\n",
    "    train_df[TARGET],\n",
    "    order=arima_order,\n",
    "    seasonal_order=seasonal_order,\n",
    "    enforce_stationarity=True,\n",
    "    enforce_invertibility=True\n",
    ")\n",
    "arima_results = arima_model.fit()\n",
    "print(arima_results.summary())\n",
    "\n",
    "# --- 2) One-step-ahead ARIMA forecasts on TRAIN ---\n",
    "train_pred = arima_results.get_prediction(\n",
    "    start=train_df.index[0],\n",
    "    end=train_df.index[-1],\n",
    "    dynamic=False\n",
    ")\n",
    "train_df['ARIMA_pred'] = train_pred.predicted_mean\n",
    "train_df.dropna(subset=['ARIMA_pred'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b602207",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # --- 3) Rolling Forecast for VALIDATION with Fixed-Length Window ---\n",
    "    val_predictions = []\n",
    "    buffer = []\n",
    "    # current_model = copy.deepcopy(arima_results)  # Work with a copy\n",
    "    current_model = arima_results  # Work with the original model\n",
    "    update_interval = 168  # Weekly updates (24*7 hours)\n",
    "    window_size = 168 * 4  # Keep 4 weeks of history for updates\n",
    "\n",
    "    for i in range(len(val_df)):\n",
    "        # 1. Forecast next step\n",
    "        fc = current_model.get_forecast(steps=1)\n",
    "        val_predictions.append(fc.predicted_mean.iloc[0])\n",
    "        \n",
    "        # 2. Store actual value in FIFO buffer\n",
    "        buffer.append(val_df[TARGET].iloc[i])\n",
    "        if len(buffer) > window_size:\n",
    "            buffer.pop(0)  # Maintain fixed window size\n",
    "            \n",
    "        # 3. Update model weekly with most recent data\n",
    "        if (i + 1) % update_interval == 0 and len(buffer) >= update_interval:\n",
    "            try:\n",
    "                # Use ONLY the latest update_interval points\n",
    "                update_data = pd.Series(\n",
    "                    buffer[-update_interval:],\n",
    "                    index=val_df.index[i+1-update_interval:i+1]\n",
    "                )\n",
    "                # Check model supports update\n",
    "                if hasattr(current_model, 'update'):\n",
    "                    current_model = current_model.update(update_data)\n",
    "                else:\n",
    "                    # Fallback: re-fit model on combined history\n",
    "                    combined_series = pd.concat([\n",
    "                        train_df[TARGET],\n",
    "                        pd.Series(buffer, index=val_df.index[:i+1])\n",
    "                    ])\n",
    "                    current_model = ARIMA(\n",
    "                        combined_series,\n",
    "                        order=arima_order,\n",
    "                        seasonal_order=seasonal_order\n",
    "                    ).fit()\n",
    "            except Exception as e:\n",
    "                print(f\"Validation update failed at step {i}: {str(e)[:200]}\")\n",
    "                buffer = buffer[-update_interval:]  # Reset buffer\n",
    "\n",
    "    val_df['ARIMA_pred'] = val_predictions\n",
    "\n",
    "    # --- 4) Rolling Forecast for TEST ---\n",
    "    # (Reuse same logic as validation with test_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nCritical error: {e}\")\n",
    "    print(\"Falling back to naive 1-lag shift\")\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['ARIMA_pred'] = df[TARGET].shift(1).fillna(method='bfill')\n",
    "\n",
    "# --- 5) Compute residuals ---\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['Residual'] = df[TARGET] - df['ARIMA_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6700e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     # --- 2) In‐sample train predictions (one‐step, dynamic=False) ---\n",
    "#     train_pred = arima_results.get_prediction(start=train_df.index[0],\n",
    "#                                           end=train_df.index[-1],\n",
    "#                                           dynamic=False)\n",
    "#     train_df['ARIMA_pred'] = train_pred.predicted_mean\n",
    "\n",
    "#     # --- 3) Rolling one‐step forecast on VALIDATION ---\n",
    "#     val_preds = []\n",
    "#     for actual in val_df[TARGET]:\n",
    "#         # 1‐step out forecast\n",
    "#         fc = arima_results.get_forecast(steps=1)\n",
    "#         p  = fc.predicted_mean.iloc[0]\n",
    "#         val_preds.append(p)\n",
    "#         # teach the filter the true next y—no refit of params\n",
    "#         arima_results = arima_results.append(endog=[actual], refit=False)\n",
    "\n",
    "#     val_df['ARIMA_pred'] = val_preds\n",
    "\n",
    "#     # --- 4) Rolling one‐step forecast on TEST ---\n",
    "#     test_preds = []\n",
    "#     for actual in test_df[TARGET]:\n",
    "#         fc = arima_results.get_forecast(steps=1)\n",
    "#         p  = fc.predicted_mean.iloc[0]\n",
    "#         test_preds.append(p)\n",
    "#         arima_results = arima_results.append(endog=[actual], refit=False)\n",
    "\n",
    "#     test_df['ARIMA_pred'] = test_preds\n",
    "\n",
    "#     print(\"Rolling forecasts complete.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Forecasting error: {e}\")\n",
    "#     print(\"Falling back to naive shift.\")\n",
    "#     for df in (train_df, val_df, test_df):\n",
    "#         df['ARIMA_pred'] = df[TARGET].shift(1).fillna(method='bfill')\n",
    "\n",
    "# # --- 5) Compute residuals ---\n",
    "# for name, df in zip(['train','val','test'], [train_df, val_df, test_df]):\n",
    "#     df['Residual'] = df[TARGET] - df['ARIMA_pred']\n",
    "#     print(f\"{name} residuals: mean={df['Residual'].mean():.1f}, std={df['Residual'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863031cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = train_df['Residual']\n",
    "\n",
    "# 1) ACF up to lag 40\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "plot_acf(residuals, lags=40, ax=ax)\n",
    "ax.set_title('ACF of ARIMA Residuals (lags 0–40)')\n",
    "plt.show()\n",
    "\n",
    "# 2) Histogram\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(residuals, bins=50, edgecolor='k')\n",
    "ax.set_title('Histogram of ARIMA Residuals')\n",
    "ax.set_xlabel('Residual value')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 3) Q–Q Plot\n",
    "fig = sm.qqplot(residuals, line='s')\n",
    "plt.title('Q–Q Plot of ARIMA Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e95381",
   "metadata": {},
   "source": [
    "## Analysis of ARIMA model residuals\n",
    "\n",
    "### 1. ACF of Residuals (lags 0–40)\n",
    "\n",
    "- **All spikes hover inside the significance bounds (except the trivial lag-0 at 1.0)**, meaning there’s essentially **no remaining autocorrelation** out to 40 lags.  \n",
    "- **Interpretation:** Your SARIMA(2,0,1)×(1,0,0,24) has done its job of removing the linear and seasonal dependence—what’s left in the residuals is, by this metric, **“white”** (uncorrelated) noise.\n",
    "\n",
    "### 2. Histogram of Residuals\n",
    "\n",
    "- You see a **bell shape centered at zero**, so the model has no systematic bias.  \n",
    "- However, the **tails** (extreme positive and negative residuals) are **fatter** than a perfect Gaussian bell would be, and you can spot a handful of large‐magnitude errors.  \n",
    "- **Interpretation:** Residuals are roughly symmetrically distributed around zero (good) but exhibit **heavy tails**, indicating occasional “spikes” or outliers that the linear SARIMA can’t explain.\n",
    "\n",
    "### 3. Q–Q Plot of Residuals\n",
    "\n",
    "- The points follow the red reference line in the center (mid‐quantiles) but **depart sharply in the upper and lower extremes**.  \n",
    "- **Interpretation:** Mid‐range residuals are roughly Gaussian, but the **extreme quantiles lie above and below** what a normal distribution predicts—again confirming **heavy tails** (and slight skew if one side deviates more).\n",
    "\n",
    "### Bottom Line\n",
    "\n",
    "- **Good news:** No autocorrelation remains—our ARIMA has captured the time‐series dependence.  \n",
    "- **Room to improve:** Residuals aren’t perfectly normal; they have heavy‐tailed outliers and maybe some heteroskedasticity.  \n",
    "- **Next step:** Hand these residuals to our LSTM-CNN. The net can learn to predict those occasional spikes and non-Gaussian patterns that our linear SARIMA can’t, tightening up your final forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55643c",
   "metadata": {},
   "source": [
    "## 4. Residual Calculation (for LSTM-CNN Input)\n",
    "\n",
    "Now, calculate the residuals for all sets (train, validation, test) using the generated ARIMA predictions. These residuals will be the target variable for the subsequent LSTM-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the residuals\n",
    "for name, df in zip(\n",
    "    ['train', 'val', 'test'],\n",
    "    [train_df, val_df, test_df]\n",
    "):\n",
    "    n_nulls = df['Residual'].isna().sum()\n",
    "    print(f\"{name.capitalize()} residuals: {n_nulls} NaN values\")\n",
    "\n",
    "# Visualize the residuals distribution for all sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(train_df['Residual'], bins=50, kde=True, label='Train', color='blue', alpha=0.5)\n",
    "sns.histplot(val_df['Residual'], bins=50, kde=True, label='Validation', color='orange', alpha=0.5)\n",
    "sns.histplot(test_df['Residual'], bins=50, kde=True, label='Test', color='green', alpha=0.5)\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residual Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b317dc",
   "metadata": {},
   "source": [
    "## 5. LSTM-CNN Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a826885",
   "metadata": {},
   "source": [
    "Scale features and create sequences (X, y) for the LSTM-CNN model. The target `y` will be the ARIMA residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa50c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Choose exogenous features (drop 'Residual' and 'year' to avoid huge scaling mismatch)\n",
    "features_exog = [\n",
    "    'hour', 'dayofweek', 'month', 'dayofyear',\n",
    "    'is_dayoff', 'temp', 'dwpt', 'rhum', 'prcp', 'wspd'\n",
    "]\n",
    "\n",
    "# 2) Extract raw arrays\n",
    "X_train_raw = train_df[features_exog].values\n",
    "X_val_raw   = val_df[features_exog].values\n",
    "X_test_raw  = test_df[features_exog].values\n",
    "\n",
    "y_train_raw = train_df['Residual'].values.reshape(-1, 1)\n",
    "y_val_raw   = val_df['Residual'].values.reshape(-1, 1)\n",
    "y_test_raw  = test_df['Residual'].values.reshape(-1, 1)\n",
    "\n",
    "# 3) Fit separate scalers\n",
    "scaler_X = StandardScaler().fit(X_train_raw)\n",
    "scaler_y = StandardScaler().fit(y_train_raw)\n",
    "\n",
    "# 4) Transform features and target\n",
    "X_train_scaled = scaler_X.transform(X_train_raw)\n",
    "X_val_scaled   = scaler_X.transform(X_val_raw)\n",
    "X_test_scaled  = scaler_X.transform(X_test_raw)\n",
    "\n",
    "y_train_scaled = scaler_y.transform(y_train_raw).flatten()\n",
    "y_val_scaled   = scaler_y.transform(y_val_raw).flatten()\n",
    "y_test_scaled  = scaler_y.transform(y_test_raw).flatten()\n",
    "\n",
    "# 5) Build sliding windows\n",
    "def create_sequences(X, y, n_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - n_steps):\n",
    "        Xs.append(X[i : i + n_steps])\n",
    "        ys.append(y[i + n_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "n_steps = 24\n",
    "\n",
    "X_train, y_train_res = create_sequences(X_train_scaled, y_train_scaled, n_steps)\n",
    "X_val,   y_val_res   = create_sequences(X_val_scaled,   y_val_scaled,   n_steps)\n",
    "X_test,  y_test_res  = create_sequences(X_test_scaled,  y_test_scaled,  n_steps)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train_res shape: {y_train_res.shape}\")\n",
    "print(f\"X_val   shape: {X_val.shape},   y_val_res   shape: {y_val_res.shape}\")\n",
    "print(f\"X_test  shape: {X_test.shape},  y_test_res  shape: {y_test_res.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee40a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Convert numpy arrays to torch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train_res, dtype=torch.float32).unsqueeze(-1)\n",
    "X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val_res,   dtype=torch.float32).unsqueeze(-1)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test_res,  dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# 2) Build TensorDatasets\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds   = TensorDataset(X_val_t,   y_val_t)\n",
    "test_ds  = TensorDataset(X_test_t,  y_test_t)\n",
    "\n",
    "# 3) Create DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=batch_size, shuffle=True,  drop_last=True\n",
    ")\n",
    "val_loader   = DataLoader(\n",
    "    val_ds,   batch_size=batch_size, shuffle=False, drop_last=False\n",
    ")\n",
    "test_loader  = DataLoader(\n",
    "    test_ds,  batch_size=batch_size, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "# Sanity check one batch\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch X shape:\", xb.shape)  # (batch_size, 24, 10)\n",
    "print(\"Batch y shape:\", yb.shape)  # (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda468ec",
   "metadata": {},
   "source": [
    "## 6. LSTM-CNN Model Definition (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d201591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hybrid LSTM-CNN model using PyTorch\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "class LSTMCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        cnn_filters,\n",
    "        kernel_size,\n",
    "        pool_size,\n",
    "        dropout_rate=0.3,\n",
    "        num_layers=1,        # ← add this\n",
    "        output_dim=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Conv blocks (same as before)…\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv1d(input_dim,   cnn_filters, kernel_size, padding=1),\n",
    "            nn.BatchNorm1d(cnn_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout1d(dropout_rate),\n",
    "            nn.MaxPool1d(pool_size)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(cnn_filters, cnn_filters, kernel_size, padding=1),\n",
    "            nn.BatchNorm1d(cnn_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout1d(dropout_rate),\n",
    "            nn.MaxPool1d(pool_size)\n",
    "        )\n",
    "\n",
    "        # compute seq_len after two poolings\n",
    "        pool_output_len = n_steps\n",
    "        for _ in range(2):\n",
    "            pool_output_len = (pool_output_len - pool_size) // pool_size + 1\n",
    "\n",
    "        # now pass num_layers into the LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_filters,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,    # ← here\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # LSTM: hn now has shape (num_layers, batch, hidden_dim)\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        last_hidden = hn[-1]\n",
    "        last_hidden = self.layer_norm(last_hidden)\n",
    "        last_hidden = self.dropout(last_hidden)\n",
    "        return self.fc(last_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim   = 128\n",
    "cnn_filters  = 64\n",
    "kernel_size  = 3\n",
    "pool_size    = 2\n",
    "dropout_rate = 0.3\n",
    "lr           = 5e-4\n",
    "weight_decay = 1e-5\n",
    "num_layers   = 2  # two‐layer LSTM\n",
    "\n",
    "model = LSTMCNN(\n",
    "    input_dim=n_features,\n",
    "    hidden_dim=hidden_dim,\n",
    "    cnn_filters=cnn_filters,\n",
    "    kernel_size=kernel_size,\n",
    "    pool_size=pool_size,\n",
    "    dropout_rate=dropout_rate,\n",
    "    num_layers=num_layers,\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef03c78",
   "metadata": {},
   "source": [
    "## 7. LSTM-CNN Training (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Training loop with early stopping\n",
    "n_epochs = 50\n",
    "patience_es = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_state = None\n",
    "history = {'train': [], 'val': []}\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # — Train —\n",
    "    model.train()\n",
    "    running_train = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_train += loss.item() * xb.size(0)\n",
    "    train_loss = running_train / len(train_loader.dataset)\n",
    "    history['train'].append(train_loss)\n",
    "\n",
    "    # — Validate —\n",
    "    model.eval()\n",
    "    running_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            running_val += criterion(preds, yb).item() * xb.size(0)\n",
    "    val_loss = running_val / len(val_loader.dataset)\n",
    "    scheduler.step(val_loss)\n",
    "    history['val'].append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch} — train: {train_loss:.6f}, val: {val_loss:.6f}\")\n",
    "\n",
    "    # LR scheduling & early stopping\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience_es:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# Restore best model\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# Plot loss curves\n",
    "plt.plot(history['train'], label='Train Loss')\n",
    "plt.plot(history['val'],   label='Val Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()\n",
    "\n",
    "# 3) Test-time inference\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        res_scaled = model(xb).cpu().numpy().flatten()\n",
    "        # invert scaling\n",
    "        res_pred = scaler_y.inverse_transform(res_scaled.reshape(-1,1)).flatten()\n",
    "        all_preds.extend(res_pred)\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "\n",
    "# 4) Add back ARIMA forecast for each test point\n",
    "#    Assuming `test_df` is indexed in the same order as your sequences:\n",
    "arima_fc = test_df['ARIMA_pred'].values[n_steps:]  # align with sequences\n",
    "final_fc = arima_fc + all_preds\n",
    "\n",
    "# 5) Compute RMSE on the test set\n",
    "true = test_df[TARGET].values[n_steps:]\n",
    "rmse = np.sqrt(mean_squared_error(true, final_fc))\n",
    "print(f\"Hybrid Test RMSE: {rmse:.2f} MW\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f997ebd",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Align test_df with your residual predictions\n",
    "#    all_preds must be a 1D array of length = len(test_df) - n_steps\n",
    "test_df_aligned = test_df.iloc[n_steps:].copy()\n",
    "\n",
    "# 2) Insert the residual forecasts and hybrid sum\n",
    "test_df_aligned['Residual_pred'] = all_preds\n",
    "test_df_aligned['Combined_pred'] = test_df_aligned['ARIMA_pred'] + test_df_aligned['Residual_pred']\n",
    "\n",
    "# 3) Define MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "# 4) Compute metrics for Hybrid\n",
    "y_true            = test_df_aligned[TARGET]\n",
    "y_pred_combined   = test_df_aligned['Combined_pred']\n",
    "\n",
    "rmse_hybrid = np.sqrt(mean_squared_error(y_true, y_pred_combined))\n",
    "mae_hybrid  = mean_absolute_error   (y_true, y_pred_combined)\n",
    "mape_hybrid = mean_absolute_percentage_error(y_true, y_pred_combined)\n",
    "\n",
    "print(\"--- Combined Model Test Set Error Metrics ---\")\n",
    "print(f\"RMSE: {rmse_hybrid:,.2f} MW\")\n",
    "print(f\"MAE:  {mae_hybrid:,.2f} MW\")\n",
    "print(f\"MAPE: {mape_hybrid:.2f}%\")\n",
    "\n",
    "# 5) Compute metrics for ARIMA-only baseline\n",
    "y_pred_arima = test_df_aligned['ARIMA_pred']\n",
    "\n",
    "rmse_arima = np.sqrt(mean_squared_error(y_true, y_pred_arima))\n",
    "mae_arima  = mean_absolute_error   (y_true, y_pred_arima)\n",
    "mape_arima = mean_absolute_percentage_error(y_true, y_pred_arima)\n",
    "\n",
    "print(\"\\n--- ARIMA Only Test Set Error Metrics ---\")\n",
    "print(f\"RMSE: {rmse_arima:,.2f} MW\")\n",
    "print(f\"MAE:  {mae_arima:,.2f} MW\")\n",
    "print(f\"MAPE: {mape_arima:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0e09b",
   "metadata": {},
   "source": [
    "## 10. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89927d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted for a period in the test set\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "test_df_aligned[TARGET].plot(ax=ax, label='Actual', style='.', alpha=0.7)\n",
    "test_df_aligned['Combined_pred'].plot(ax=ax, label='Combined Forecast', style='.', alpha=0.7)\n",
    "# test_df_aligned['ARIMA_pred'].plot(ax=ax, label='ARIMA Forecast', style='--', alpha=0.7)\n",
    "ax.set_title('Actual vs. Combined Forecast (Test Set)')\n",
    "ax.set_ylabel('MW')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Zoom in on a specific week\n",
    "start_date = test_df_aligned.index.min() + pd.Timedelta(days=7)\n",
    "end_date = start_date + pd.Timedelta(days=7)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "test_df_aligned.loc[start_date:end_date, TARGET].plot(ax=ax, label='Actual', style='.-')\n",
    "test_df_aligned.loc[start_date:end_date, 'Combined_pred'].plot(ax=ax, label='Combined Forecast', style='-')\n",
    "ax.set_title(f'Forecast vs Actuals ({start_date.date()} to {end_date.date()})')\n",
    "ax.set_ylabel('MW')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c507e",
   "metadata": {},
   "source": [
    "## 11. Error Analysis: Worst/Best Predicted Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error and absolute error for each prediction in the aligned test set\n",
    "test_df_aligned['error'] = test_df_aligned[TARGET] - test_df_aligned['Combined_pred']\n",
    "test_df_aligned['abs_error'] = test_df_aligned['error'].abs()\n",
    "\n",
    "# Group by date and calculate mean errors\n",
    "error_by_day = test_df_aligned.groupby(test_df_aligned.index.date) \\\n",
    "    .agg(mean_Actual_MW=('PJME_MW', 'mean'),\n",
    "         mean_Combined_Pred=('Combined_pred', 'mean'),\n",
    "         mean_error=('error', 'mean'),\n",
    "         mean_abs_error=('abs_error', 'mean'))\n",
    "\n",
    "# Add weekday name for context\n",
    "error_by_day['weekday'] = pd.to_datetime(error_by_day.index).strftime('%A')\n",
    "\n",
    "print(\"--- Top 10 Days with Worst Mean Absolute Error ---\")\n",
    "worst_days = error_by_day.sort_values('mean_abs_error', ascending=False).head(10)\n",
    "worst_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d96e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Top 10 Days with Best Mean Absolute Error ---\")\n",
    "best_days = error_by_day.sort_values('mean_abs_error', ascending=True).head(10)\n",
    "best_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfcffe0",
   "metadata": {},
   "source": [
    "### Worst Predicted Day Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the date with the highest mean absolute error\n",
    "worst_day_date = error_by_day['mean_abs_error'].idxmax()\n",
    "worst_day_str = worst_day_date.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Plotting worst predicted day: {worst_day_str}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "test_df_aligned.loc[worst_day_str][TARGET].plot(ax=ax, label='Actual', style='.-')\n",
    "test_df_aligned.loc[worst_day_str]['Combined_pred'].plot(ax=ax, label='Combined Prediction', style='-')\n",
    "ax.set_title(f'Worst Predicted Day ({worst_day_str}) - MAE: {error_by_day.loc[worst_day_date, \"mean_abs_error\"]:.2f} MW')\n",
    "ax.set_ylabel('MW')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ae0c4",
   "metadata": {},
   "source": [
    "### Best Predicted Day Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debcc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the date with the lowest mean absolute error\n",
    "best_day_date = error_by_day['mean_abs_error'].idxmin()\n",
    "best_day_str = best_day_date.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Plotting best predicted day: {best_day_str}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "test_df_aligned.loc[best_day_str][TARGET].plot(ax=ax, label='Actual', style='.-')\n",
    "test_df_aligned.loc[best_day_str]['Combined_pred'].plot(ax=ax, label='Combined Prediction', style='-')\n",
    "ax.set_title(f'Best Predicted Day ({best_day_str}) - MAE: {error_by_day.loc[best_day_date, \"mean_abs_error\"]:.2f} MW')\n",
    "ax.set_ylabel('MW')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764970d5",
   "metadata": {},
   "source": [
    "### 📉 Worst‐predicted days  \n",
    "| Date       | Mean Load | Mean Forecast | Mean Abs Error | Weekday  | Notes |\n",
    "|------------|-----------|---------------|---------------:|----------|-------|\n",
    "| **2018-07-06** | 39 145 MW  | 39 517 MW      | 686 MW        | Friday   | **Day after Independence Day**—holiday rebound, atypical ramp-up pattern.  \n",
    "| 2018-07-15 | 34 204 MW  | 34 233 MW      | 609 MW        | Sunday   | Midsummer weekend—higher residential AC load.  \n",
    "| 2018-07-05 | 42 402 MW  | 42 267 MW      | 568 MW        | Thursday | Pre-holiday warm-up.  \n",
    "| 2018-05-15 | 31 482 MW  | 31 359 MW      | 541 MW        | Tuesday  | Early-summer heat spike.  \n",
    "\n",
    "Two of the top three misses straddle **July 4th**—clear evidence that your holiday-and-weekend flag isn’t fully capturing the “day + 1/−1” effects. Likewise, several early-summer heat days (May/June) slip through.\n",
    "\n",
    "### 📈 Best‐predicted days  \n",
    "| Date       | Mean Load | Mean Forecast | Mean Abs Error | Weekday  |\n",
    "|------------|-----------|---------------|---------------:|----------|\n",
    "| 2017-04-24 | 26 415 MW  | 26 379 MW      | 115 MW        | Monday   |\n",
    "| 2018-01-28 | 27 357 MW  | 27 307 MW      | 117 MW        | Sunday   |\n",
    "| 2017-05-22 | 26 889 MW  | 26 867 MW      | 125 MW        | Monday   |\n",
    "\n",
    "These are **steady, off-peak periods** (spring/fall shoulder months) where both ARIMA and your CNN–LSTM residual learner have no surprises—loads follow their usual daily cycle.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Next steps\n",
    "\n",
    "1. **Enhance your holiday feature**  \n",
    "   - Add “day + 1” and “day − 1” flags around federal holidays so the network sees those rebound effects explicitly.  \n",
    "\n",
    "2. **Bake in extreme-temperature signals**  \n",
    "   - Rather than raw temp, include features like “Cooling Degree Hours above 75°F” or quadratic terms to better capture AC load surges on hot days.  \n",
    "\n",
    "3. **Special‐case gating**  \n",
    "   - For the handful of days with MAE > 500 MW, consider a tiny “holiday‐heat” submodel: e.g. train a small decision‐tree on top of your residuals to correct whenever (is_holiday_adjacent & temp > 85°F).  \n",
    "\n",
    "4. **Granular error analysis**  \n",
    "   - Drill into those worst days hour by hour. Perhaps the network misses morning ramp vs. evening ramp differently—if so, you could add “hour×dayoff” interaction features.  \n",
    "\n",
    "5. **Re-validate after tweaks**  \n",
    "   - Recompute your day-level MAE table and confirm that July 5–6 errors shrink from ~650 MW down toward your average ~250 MW.  \n",
    "\n",
    "By encoding a bit more domain knowledge around **holiday adjacency** and **temperature extremes**, you’ll likely eliminate those “spikes” in your worst‐case errors—pushing your hybrid MAPE even lower than 0.9%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60305ef0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hourly-energy-consumption-p0ggDBMx-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
